# Using custom env
# Karel custom env: cleanHouse, harvester, fourCorners, randomMaze, stairClimber, topOff, doorkey, oneStroke, seeder, snake
# class ExecEnv_option
# class ProgramEnv_option (task_definition == custom_reward)

# PPO for harvester (64 dim)
CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task harvester --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_harvester_L38_step5_dim64u256_recurrent_fixedInput --seed 123 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.05 --PPO.hidden_size 64 --PPO.decoder_deterministic True --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_easy_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True

# PPO for cleanHouse (64 dim)
#CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task cleanHouse --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_cleanHouse_L38_step5_dim64u256_recurrent_fixedInput --seed 123 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.05 --PPO.hidden_size 64 --PPO.decoder_deterministic True --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_easy_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True --input_height 14 --input_width 22


# PPO for fourCorners (64 dim)
#CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task fourCorners --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_fourCorners_L38_step5_dim64u256_recurrent_fixedInput --seed 123 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.05 --PPO.hidden_size 64 --PPO.decoder_deterministic True --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_easy_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True

# PPO for randomMaze (64 dim)
#CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task randomMaze --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_randomMaze_L38_step5_dim64u256_recurrent_fixedInput --seed 123 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.05 --PPO.hidden_size 64 --PPO.decoder_deterministic True --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_easy_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True

# PPO for stairClimber_sparse (64 dim)
#CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task stairClimber_sparse --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_stairClimber_sparse_L38_step5_dim64u256_recurrent_fixedInput --seed 123 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.05 --PPO.hidden_size 64 --PPO.decoder_deterministic True --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_easy_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True

# PPO for topOff_sparse (64 dim)
#CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task topOff_sparse --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_topOff_sparse_L38_step5_dim64u256_recurrent_fixedInput --seed 123 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.05 --PPO.hidden_size 64 --PPO.decoder_deterministic True --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_easy_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True

# PPO for doorkey (64dim)
#CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task doorkey --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_doorkey_fixedInitPos_L38_step5_dim64u256_recurrent_fixedInput --seed 123 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.05 --PPO.hidden_size 64 --PPO.decoder_deterministic True --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_hard_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True

# PPO for oneStroke (64 dim)
#CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task oneStroke --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_oneStroke_randomInitPos_penalty0p1_L38_step5_dim64u256_recurrent_fixedInput_entropyp1_s9487 --seed 9487 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.1 --PPO.hidden_size 64 --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_hard_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True

# PPO for seeder (64dim)
#CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task seeder --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_seeder_fixedInitPos_L38_step5_dim64u256_recurrent_fixedInput --seed 123 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.05 --PPO.hidden_size 64 --PPO.decoder_deterministic True --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_hard_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True

# PPO for snake (64dim)
#CUDA_VISIBLE_DEVICES="0" python3 pretrain/trainer_option_new_vae_L30.py --configfile pretrain/cfg_option_new_vae.py --datadir placeholder --mdp_type ProgramEnv_option_new_vae_v2_key2door_fixed  --num_lstm_cell_units 64 --algorithm PPO_option --net.saved_params_path pretrain/output_dir_new_vae_L40_1m_30epoch_20230104/LEAPSL_tanh_epoch30_L40_1m_h64_u256_option_latent_p1_gru_linear-handwritten-123-20230110-110800/best_valid_params.ptp --net.num_rnn_encoder_units 256 --net.num_rnn_decoder_units 256 --net.use_linear True --net.latent_mean_pooling False  --env_task snake --rl.envs.executable.task_definition custom_reward  --max_program_len 40 --dsl.max_program_len 40 --prefix PPO_option_snake_fixedInitPos_L38_step5_dim64u256_recurrent_fixedInput --seed 123 --PPO.num_processes 16 --PPO.lr 1e-5 --PPO.num_steps 800 --PPO.num_mini_batch 10 --PPO.num_env_steps 100e6 --PPO.entropy_coef 0.05 --PPO.hidden_size 64 --PPO.decoder_deterministic True --log_interval 1 --save_interval 50 --log_video_interval 200 --outdir pretrain/output_dir_ppo_karel_hard_L38 --max_episode_steps 5 --input_channel 8 --logging.wandb False --PPO.recurrent_policy True --fixed_input True
